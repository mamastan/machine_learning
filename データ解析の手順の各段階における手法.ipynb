{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "# データ解析の手順の各段階における手法\n",
    "\n",
    "下記URLのまとめ。\n",
    "\n",
    "- http://univprof.com/archives/16-02-11-2849465.html\n",
    "- http://univprof.com/archives/16-05-01-2850729.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 数値化されたデータセットを準備する \n",
    "\n",
    "最初にデータセットを準備します。\n",
    "データ解析で扱うためにはデータセットが数字で表されている必要があります。\n",
    "エクセルのファイルに数値が並んでいるイメージです。最初からデータが数字で表されていれば良いですが、そうでない場合は、適切に数値化する必要があります。\n",
    "\n",
    "例えば、文字データや化学構造のデータや、画像データなどは、目で見て確認できますが、数値では表されていないため、適切に数値化する必要があります。\n",
    "\n",
    "データ数が多い場合はあらかじめデータを選択しておくこともあります。\n",
    " \n",
    "#### 【手法】\n",
    "- Random sampling \n",
    "- Systematic sampling \n",
    "- Kennard-Stone (KS) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## データを見える化 (可視化) する\n",
    "例えばエクセルファイルに数値が並んだものなど、準備されたデータセットがどんな様子なのか、データセット見ただけではよくわかりません。\n",
    "\n",
    "そこで数値化されたデータを見える化します。可視化といったりもします。\n",
    "\n",
    "例えば、最大値・最小値・平均値・中央値・標準偏差などの統計量を確認したり、ヒストグラムを確認したりします。\n",
    "変数、もしくは記述子や特徴量 が２つある場合はさらに、変数間の相関係数を調べたり、横軸をある変数にして縦軸をもう一つの変数にしてデータをプロットしたりして確認できます。\n",
    "\n",
    "変数の数がそれ以上の多次元 (多変量) のデータ、つまり変数の数が多いデータ、では、ロットして全体の様子を確認することは難しいため、例えば二次元などに低次元化してデータの様子を確認します。\n",
    "\n",
    "この見える化 もしくは可視化 について、次の外れ値処理・変数処理・ノイズ処理を行った後に、再度行うことで、より適切にデータの様子を確認できることがあります。\n",
    "\n",
    "#### 【手法】\n",
    "- 基礎統計量 [平均値、中央値、最大値、最小値、標準偏差、相関係数など] \n",
    "- ヒストグラム \n",
    "- 主成分分析 (Principal Component Analysis, PCA) \n",
    "- 独立成分分析 (Independent Component Analysis, ICA) \n",
    "- Kernel Principal Component Analysis (KPCA) \n",
    " -Kernel Independent Component (KICA) \n",
    "- 自己組織化写像 (Self-Organizing Map, SOM) \n",
    "- Generative Topographic Map (GTM) \n",
    "- Stochastic Neighbor Embedding (SNE) \n",
    "- t-distributed Stochastic Neighbor Embedding (t-SNE) \n",
    "- 多次元尺度構成法 (Multi-Dimensional Scaling, MSD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 外れ値処理・変数処理・ノイズ処理を行う\n",
    "データを見える化すると、\n",
    "使っている変数が意味のある変数かどうか、\n",
    "データに外れ値があるかどうか、\n",
    "を判断できるようになります。\n",
    "例えば標準偏差が0の変数や、\n",
    "関係数が1の変数の組における一方の変数を\n",
    "削除してもデータセット内の情報量は変わりません。\n",
    "またヒストグラムを見て、\n",
    "他のデータと大きく異なる値を持つデータは、\n",
    "後の解析に悪影響を及ぼす可能性があるため、\n",
    "外れ値として削除するか検討したほうがよいです。\n",
    "またスペクトルデータや時系列データを扱う際は、\n",
    "ノイズ処理を行うこともあります。\n",
    "\n",
    "#### 【手法】\n",
    "##### 外れ値処理 \n",
    "- ３シグマ法 \n",
    "- Hampel filter もしくは Hampel idenifer \n",
    "- 独立成分分析 (Independent Component Analysis, ICA) \n",
    "- 移動平均 \n",
    " -Savitzky-Golay (SG) filter \n",
    " \n",
    "##### 変数処理 \n",
    "- 標準偏差の小さい変数を削除 \n",
    "- 同じ値を持つデータの割合の大きい変数を削除 \n",
    "- 相関係数の高い変数の組の一方を削除 \n",
    "- ステップワイズ回帰 (Stepwise Regression, SR) \n",
    "- Variable Importance in Projection (VIP) \n",
    "- Least Absolute Shrinkage and Selection Operator (LASSO) \n",
    "- 決定木 (Decision Tree, DT) \n",
    "- ランダムフォレスト (Random Forests, RF) \n",
    "- 組み合わせ最適化法 \n",
    "- Genetic Algorithm-based Partial Least Squares (GAPLS)\n",
    "\n",
    "##### ノイズ処理 \n",
    "- 移動平均 \n",
    "- Savitzky-Golay (SG) filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### クラスタリングを行う\n",
    "データセットの中には様々なデータが含まれています。\n",
    "データを見える化することによって、データが特徴的に分布しているのを確認できます。\n",
    "\n",
    "ここではデータセットを、データ間の類似度にもとづいてのかたまり (クラスター)ごとに分けます。これによりデータ内の構造が分かり、クラスタリングの結果が有用な知見となります。\n",
    " \n",
    " #### 【手法】\n",
    "- 階層的クラスタリング \n",
    "- k-means法 \n",
    "- Affinity Propagation (AP) \n",
    "- Density-Based Spatial Clustering of Applications with Noise(DBSCAN) \n",
    "##### 類似度に関連する指標 \n",
    "- ユークリッド距離 \n",
    "- マハラノビス距離 \n",
    "- コサイン類似度 \n",
    "- 相関係数 \n",
    "- タニモト係数 (tanimoto similarity) \n",
    "- 相互情報量 \n",
    "- カルバック・ライブラー情報量 (Kullback-Leibler divergence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回帰モデルもしくはクラス分類モデルを作る\n",
    "目的変数と説明変数との間の関係をモデル化します。\n",
    "\n",
    "目的変数が連続値の場合は回帰分析、目的変数が離散値 (カテゴリ値) の場合はクラス分類分析になり、それぞれ回帰モデル・クラス分類モデルを作ります。\n",
    "\n",
    "なおモデル作成の前に、データセットをモデルを構築するためのデータセット(モデル構築用データセット) と作成されたモデルを検証するためのデータセット(モデル検証用データセット、ブラインドデータセット) に分けることが一般的です。\n",
    "あたかもモデル検証用データセットの目的変数の値が、わからないと仮定して予測を行い、実際の目的変数の値と予測された目的変数の値との、一致具合を検証します。\n",
    " \n",
    "#### 【手法】\n",
    " - 最小二乗法による重回帰 (Multiple Linear Regression, MLR  もしくは Ordinary Linear Regression, OLR) \n",
    " - 主成分回帰 (Principal Component Regression, PCR) \n",
    " - 部分的最小二乗法 (Partial Least Squares, PLS) \n",
    " - Kernel Partial Least Squares (KPLS) \n",
    " - 正準相関解析 (Canonical Correlation Analysis, CCA) \n",
    " - リッジ回帰 (Ridge Regression, RR) \n",
    " - Least Absolute Shrinkage and Selection Operator (LASSO) \n",
    " - k近傍法 (k-Nearest Neighbor algorithm, k-NN) \n",
    " - 決定木 (Decision Tree, DT) \n",
    " - ランダムフォレスト (Random Forest, RF) \n",
    " - 線形判別分析 (Linear Discriminant Analysis, LDA) \n",
    " - サポートベクターマシン (Support Vector Machine, SVM) \n",
    " - サポートベクター回帰 (Support Vector Regression, SVR) \n",
    " - Backpropagation neural network \n",
    " - Counterpropagation neural network \n",
    " - ディープラーニング \n",
    " - Gaussian Process (GP) \n",
    " - Adaptive Boosting (AdaBoost) \n",
    "\n",
    "#### 外れ値処理・変数処理・ノイズ処理を行いながら回帰モデルもしくはクラス分類モデルを完成させる。\n",
    "いつも最初から良好なモデルが得られるわけではありません。\n",
    "外れ値の削除、変数選択、変数変換、スムージングなどを行いながら予測性能の高い回帰モデルもしくはクラス分類モデルを作成することを目指します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 回帰モデルもしくはクラス分類モデルを適用できるデータ領域を決める\n",
    "回帰モデルやクラス分類モデルが作成されると、基本的にはどんな説明変数のデータもモデルに入力することができ、目的変数の値を予測することができます。\n",
    "\n",
    "しかし、その予測された値を信頼して良いかどうかは話が別です。\n",
    "モデルを構築した際に用いたデータと、非常に類似したデータであれば、モデル構築用データセットにおける誤差 (回帰の場合)、 もしくはエラー率 (クラス分類の場合) と、同じくらいのものを期待することができますが、類似しなくなればなるほどそれと同じような精度は期待できなくなってしまいます。\n",
    "\n",
    "モデル構築用データセットにおけるモデルの精度と、同じ精度を期待できるデータ領域を決めたり、予測したいデータごとに期待される誤差、もしくはエラー率を推定する仕組みを構築します。\n",
    "\n",
    " #### 【手法】\n",
    "- データの範囲 (range) \n",
    "- 平均からの距離 \n",
    "- k近傍法 (k-Nearest Neighbor algorithm, k-NN) \n",
    "- One-Class Support Vector Machine (OCSVM) \n",
    "- Gaussian Process (GP)      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
